---
layout: post
title: Spark安装指南
date: 2020-07-15
categories: bigdata spark
tags: spark 安装指南
permalink: install_spark
published: true
---

jdk 和 haodop 在之前的文章 [Hadoop安装指南](install_hadoop) 详细介绍过安装方法，这里就不赘述了。本文记录 scala 以及 spark 的安装，采用 standalone 模式。

## 安装环境

- ubuntu-20.04.3 LTS

- jdk-1.8.0_321

- scala-2.12.15

- hadoop-2.10.1

- spark-3.1.2

## 安装 scala

在 [Scala 官网](https://www.scala-lang.org) 下载所需版本的 scala 安装包，解压到 `/usr/local` 文件夹中，并在 `~/.bashrc` 中设置环境变量。

```bash
# set scala home
export SCALA_HOME=/usr/local/scala-2.12
export PATH=$SCALA_HOME/bin:$PATH
```

## 安装 spark

在 [Spark 官网](http://spark.apache.org) 下载所需要版本的 spark 安装包，解压到 `/usr/local` 文件夹中，并在 `~/.bashrc` 中设置环境变量。

```bash
# set spark home.
export SPARK_HOME=/usr/local/spark
export SPARK_YARN_USER_ENV=${HADOOP_CONF_DIR}
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
```

## 修改 spark 配置文件

### spark-env.sh

```bash
export JAVA_HOME=/usr/local/jdk1.8
export SCALA_HOME=/usr/local/scala-2.12
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_MASTER_HOST=localhost
```

### slaves

```conf
localhost
```

## 启动 spark

### standalone 模式启动

```bash
# 启动 spark 自带的资源管理器
$SPARK_HOME/sbin/start-all.sh
```

- 使用 `jps` 命令查看进程是否启动

  如果 spark 成功启动则会有以下 2 个进程。

  ```conf
  Master
  Worker
  ```
- Web UI 查看集群状态

  [`http://localhost:8080/`](http://localhost:8080/)

### standalone 模式连接 spark

```bash
$SPARK_HOME/bin/spark-shell --master spark://localhost:7077
```

### yarn 模式连接 spark

```bash
$SPARK_HOME/bin/spark-shell --master yarn
```
