---
layout: post
title: Hadoop安装指南
date: 2020-07-14
categories: bigdata hadoop
tags: Hadoop 安装指南
permalink: install_hadoop
published: true
---

笔者从事大数据相关的工作，说不清什么原因，就是想自己搞一个 hadoop 集群，本文来记录一下伪分布式下 hadoop 最精简的安装。

## 安装环境

- ubuntu-20.04.3 LTS

- jdk-1.8.0_321

- hadoop-2.10.1

## 配置 ssh 无密登陆

```bash
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub qzp@localhost
```

## 安装 jdk

在 [Oracle 官网](https://www.oracle.com/index.html) 下载所需要版本的 jdk 安装包，解压到 `/usr/local` 文件夹中，并在 `~/.bashrc` 中设置环境变量。

```bash
# set java home
export JAVA_HOME=/usr/local/jdk1.8
export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATH
export PATH=$JAVA_HOME/bin:$PATH
```

## 安装 hadoop

在 [Apache Hadoop 官网](http://hadoop.apache.org) 下载所需要版本的 hadoop 安装包，解压到 `/usr/local` 文件夹中，在 `~/.bashrc` 中设置环境变量。

```bash
# set hadoop home
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$HADOOP_HOME/bin:$PATH
```

## 修改 hadoop 配置文件

hadoop 的配置文件都在 `$HADOOP_HOME/etc/hadoop` 文件夹中。

### hadoop_env.sh

```bash
export JAVA_HOME=/usr/local/jdk1.8
```

### core-site.xml

```xml
<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/tmp/hadoop</value>
  </property>
  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
  </property>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

### hdfs-site.xml

```xml
<configuration>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>localhost:50070</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>localhost:50090</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
    <description>nodes total count</description>
  </property>
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>file:///usr/local/hadoop/data/dfs/name</value>
     <final>true</final>
  </property>
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>file:///usr/local/hadoop/data/dfs/data</value>
     <final>true</final>
  </property>
  <property>
     <name>dfs.namenode.checkpoint.dir</name>
     <value>file:///usr/local/hadoop/data/dfs/namesecondary</value>
     <final>true</final>
  </property>
</configuration>
```

### mapred-site.xml

```xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>localhost:10020</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>localhost:19888</value>
  </property>
  <property>
    <name>mapreduce.map.memory.mb</name>
    <value>4096</value>
  </property>
</configuration>
```

### yarn-site.xml

```xml
<configuration>
<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>localhost:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>localhost:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>localhost:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>localhost:8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>localhost:8088</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
  </property>
</configuration>
```

### slaves

```conf
localhost
```

## 格式化 namenode

```bash
$HADOOP_HOME/bin/hdfs namenode -format
```

如果格式化成功则会看到 `common.Storage: Storage directory /usr/local/hadoop/data/dfs/name has been successfully formatted.` 的提示。

## 启动 hadoop

```bash
$HADOOP_HOME/sbin/start-all.sh
```

- 使用 `jps` 命令查看进程是否启动

  如果 hadoop 成功启动则会有以下 5 个进程。

  ```conf
  DataNode
  NodeManager
  ResourceManager
  NameNode
  SecondaryNameNode
  ```

- Web UI 查看集群状态

  [`http://localhost:50070/`](http://localhost:50070/) 查看 namenode 和 datanode 是否启动成功

  [`http://localhost:8088/`](http://localhost:8088/) 查看 yarn 是否启动成功

- 查看端口占用情况

  `netstat -ntupl`
